Shipping things on [llama.cpp](https://github.com/ggml-org/llama.cpp):

- The brand new `llama-cli`: https://github.com/ggml-org/llama.cpp/pull/17824
- Serving multiple models in parallel via `llama-server`: https://github.com/ggml-org/llama.cpp/pull/17470
- Shipping Ministral 3, Devstral 2 (partnership with Mistral): https://github.com/ggml-org/llama.cpp/pull/17644
- Shipping GPT-OSS (partnership with OpenAI and GGML team): https://github.com/ggml-org/llama.cpp/pull/15091
- Bring vision support to `llama-server`: https://github.com/ggml-org/llama.cpp/pull/12898

<details>
<summary>and more...</summary>

- (Big) refactoring vision support in llama.cpp, introducing `libmtmd`: https://github.com/ggml-org/llama.cpp/pull/12849
- Support various vision models: [Pixtral](https://github.com/ggml-org/llama.cpp/pull/13065), [SmolVLM](https://github.com/ggml-org/llama.cpp/pull/13050), etc (check out this [viral demo](https://x.com/ngxson/status/1921980096421806127))
- Support various vision models: [Pixtral](https://github.com/ggml-org/llama.cpp/pull/13065), [SmolVLM](https://github.com/ggml-org/llama.cpp/pull/13050), etc (check out this [viral demo](https://x.com/ngxson/status/1921980096421806127))
- Gemma 3 Vision support (partnership with Google): https://github.com/ggml-org/llama.cpp/pull/12343
- WASM speed improvement: https://github.com/ggml-org/llama.cpp/pull/11453 (also checkout [wllama](https://github.com/ngxson/wllama))
- Refactor argument parser, for a better CLI UX: https://github.com/ggml-org/llama.cpp/pull/9308
- Revamp llama.cpp Web UI (currently deprecated, but was the precursor for the [modern version](https://github.com/ggml-org/llama.cpp/discussions/16938)): https://github.com/ggml-org/llama.cpp/pull/10175
- llama.cpp <> Hugging Face inference endpoint integration: [read docs](https://huggingface.co/docs/inference-endpoints/en/guides/llamacpp_container)
- Hot-swapping LoRA adapters: https://github.com/ggml-org/llama.cpp/pull/8332

</details>
